{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb08f6d0",
   "metadata": {},
   "source": [
    "Here's a Python script to:\n",
    "\n",
    "Load prediction files from both models.\n",
    "\n",
    "Load ground truth.\n",
    "\n",
    "Compare their formats (number of columns, data types, column meanings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e46867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== FORMAT CHECK ==========\n",
      "\n",
      "YOLOv9 Prediction shape: (17, 6)\n",
      "Sample row from YOLOv9 Prediction: [0.        0.462345  0.633205  0.0858389 0.0691288 0.558853 ]\n",
      "ðŸ” YOLOv9 Prediction appears to be a **prediction** with 6 columns:\n",
      "   Likely format: [class_id, x_center, y_center, width, height, confidence]\n",
      "\n",
      "KD-YOLO-ViT Prediction shape: (292, 6)\n",
      "Sample row from KD-YOLO-ViT Prediction: [0.       0.462891 0.631641 0.090625 0.072656 0.686064]\n",
      "ðŸ” KD-YOLO-ViT Prediction appears to be a **prediction** with 6 columns:\n",
      "   Likely format: [class_id, x_center, y_center, width, height, confidence]\n",
      "\n",
      "Ground Truth shape: (1, 0)\n",
      "Sample row from Ground Truth: []\n",
      "âš ï¸ Ground Truth has 0 columns, format unknown. Please inspect manually.\n",
      "\n",
      "âœ… Prediction formats match. You can proceed to ensemble.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_19040\\1945626783.py:5: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels/0045.txt\"\n",
      "  data = np.loadtxt(file_path)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_predictions(file_path):\n",
    "    try:\n",
    "        data = np.loadtxt(file_path)\n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(1, -1)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def describe_format(name, data):\n",
    "    if data is None or data.shape[0] == 0:\n",
    "        print(f\"{name}: No data found or file empty.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"{name} shape: {data.shape}\")\n",
    "    sample = data[0]\n",
    "    print(f\"Sample row from {name}: {sample}\")\n",
    "\n",
    "    num_columns = data.shape[1]\n",
    "\n",
    "    if num_columns == 6:\n",
    "        print(f\"ðŸ” {name} appears to be a **prediction** with 6 columns:\")\n",
    "        print(\"   Likely format: [class_id, x_center, y_center, width, height, confidence]\")\n",
    "    elif num_columns == 5:\n",
    "        print(f\"ðŸ“Œ {name} appears to be a **ground truth** with 5 columns:\")\n",
    "        print(\"   Likely format: [class_id, x_center, y_center, width, height]\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {name} has {num_columns} columns, format unknown. Please inspect manually.\")\n",
    "\n",
    "    print()\n",
    "\n",
    "def check_all_formats(yolo9_pred, kdvit_pred, gt):\n",
    "    print(\"========== FORMAT CHECK ==========\\n\")\n",
    "    describe_format(\"YOLOv9 Prediction\", yolo9_pred)\n",
    "    describe_format(\"KD-YOLO-ViT Prediction\", kdvit_pred)\n",
    "    describe_format(\"Ground Truth\", gt)\n",
    "    \n",
    "    # Check if predictions can be ensembled directly\n",
    "    if yolo9_pred is not None and kdvit_pred is not None:\n",
    "        if yolo9_pred.shape[1] != kdvit_pred.shape[1]:\n",
    "            print(\"âŒ Prediction formats mismatch! Cannot ensemble directly.\")\n",
    "        else:\n",
    "            print(\"âœ… Prediction formats match. You can proceed to ensemble.\")\n",
    "\n",
    "# ======= Example usage =======\n",
    "yolov9_pred_file = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/yolov9e_predictions/0045.txt'\n",
    "kd_yolo_vit_pred_file = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/kd-yolox-vit_predictions/0045.txt'\n",
    "ground_truth_file = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels/0045.txt'\n",
    "\n",
    "yolov9_preds = load_predictions(yolov9_pred_file)\n",
    "kd_yolo_vit_preds = load_predictions(kd_yolo_vit_pred_file)\n",
    "ground_truth = load_predictions(ground_truth_file)\n",
    "\n",
    "check_all_formats(yolov9_preds, kd_yolo_vit_preds, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4672deb",
   "metadata": {},
   "source": [
    "ðŸ” You currently have: [class_id, x_center, y_center, width, height, confidence]\n",
    "For NMS convert: [x1, y1, x2, y2, confidence, class_id]\n",
    "\n",
    "x1 = x_center - width / 2\n",
    "\n",
    "y1 = y_center - height / 2\n",
    "\n",
    "x2 = x_center + width / 2\n",
    "\n",
    "y2 = y_center + height / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68aa578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error processing 0045.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 0050.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 0057.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 0062.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 0077.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 0089.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 0095.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 0098.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 1-0047.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 1-0075.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 1-0089.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 2-0030.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 2-0039.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 2-0056.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 2-0075.txt: not enough values to unpack (expected 5, got 0)\n",
      "âŒ Error processing 2-0087.txt: not enough values to unpack (expected 5, got 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\0045.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\0050.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\0057.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\0062.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\0077.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\0089.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\0095.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\0098.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\1-0047.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\1-0075.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\1-0089.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\2-0030.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\2-0039.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\2-0056.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\2-0075.txt\"\n",
      "  data = np.loadtxt(input_path)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17996\\2528806980.py:29: UserWarning: loadtxt: input contained no data: \"C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels\\2-0087.txt\"\n",
      "  data = np.loadtxt(input_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_corners(data, is_prediction=True):\n",
    "    \"\"\"Convert [class_id, cx, cy, w, h, (conf)] -> [class_id, x1, y1, x2, y2, (conf)]\"\"\"\n",
    "    converted = []\n",
    "    for row in data:\n",
    "        class_id, cx, cy, w, h = row[:5]\n",
    "        x1 = cx - w / 2\n",
    "        y1 = cy - h / 2\n",
    "        x2 = cx + w / 2\n",
    "        y2 = cy + h / 2\n",
    "        if is_prediction:\n",
    "            conf = row[5]\n",
    "            converted.append([class_id, x1, y1, x2, y2, conf])\n",
    "        else:\n",
    "            converted.append([class_id, x1, y1, x2, y2])\n",
    "    return np.array(converted)\n",
    "\n",
    "def process_folder(input_folder, output_folder, is_prediction=True):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for file in os.listdir(input_folder):\n",
    "        if not file.endswith('.txt'):\n",
    "            continue\n",
    "        input_path = os.path.join(input_folder, file)\n",
    "        output_path = os.path.join(output_folder, file)\n",
    "\n",
    "        try:\n",
    "            data = np.loadtxt(input_path)\n",
    "            if data.ndim == 1:\n",
    "                data = data.reshape(1, -1)\n",
    "            converted = convert_to_corners(data, is_prediction=is_prediction)\n",
    "            np.savetxt(output_path, converted, fmt='%.6f')\n",
    "            #print(f\"âœ… Converted and saved: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {file}: {e}\")\n",
    "\n",
    "# === File paths ===\n",
    "yolov9_input = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/yolov9e_predictions'\n",
    "kdvit_input = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/kd-yolox-vit_predictions'\n",
    "gt_input = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/test_labels'\n",
    "\n",
    "yolov9_output = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/yolov9_corners'\n",
    "kdvit_output = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/kdvit_corners'\n",
    "gt_output = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/gt_corners'\n",
    "\n",
    "# === Run Conversion ===\n",
    "process_folder(yolov9_input, yolov9_output, is_prediction=True)\n",
    "process_folder(kdvit_input, kdvit_output, is_prediction=True)\n",
    "process_folder(gt_input, gt_output, is_prediction=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba1af8",
   "metadata": {},
   "source": [
    "Improved ensemble by nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc4179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from collections import defaultdict\n",
    "\n",
    "def weighted_box_fusion(predictions_list, model_weights, iou_thresh=0.5, conf_thresh=0.1):\n",
    "    \"\"\"\n",
    "    Apply Weighted Box Fusion (WBF) to combine predictions from multiple models\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions_list: List of numpy arrays, each containing predictions from one model\n",
    "                       Each prediction has format [class_id, x1, y1, x2, y2, conf]\n",
    "    - model_weights: List of weights for each model\n",
    "    - iou_thresh: IoU threshold for clustering boxes\n",
    "    - conf_thresh: Confidence threshold for filtering weak predictions\n",
    "    \n",
    "    Returns:\n",
    "    - Array of fused predictions [class_id, x1, y1, x2, y2, conf]\n",
    "    \"\"\"\n",
    "    # Filter empty predictions and apply confidence threshold\n",
    "    filtered_preds = []\n",
    "    filtered_weights = []\n",
    "    \n",
    "    for i, preds in enumerate(predictions_list):\n",
    "        if len(preds) > 0:\n",
    "            mask = preds[:, 5] >= conf_thresh\n",
    "            if np.any(mask):\n",
    "                filtered_preds.append(preds[mask])\n",
    "                filtered_weights.append(model_weights[i])\n",
    "    \n",
    "    if not filtered_preds:\n",
    "        return np.empty((0, 6))\n",
    "    \n",
    "    # Convert all to tensors\n",
    "    pred_tensors = [torch.tensor(p, dtype=torch.float32) for p in filtered_preds]\n",
    "    \n",
    "    # Process each class separately\n",
    "    final_predictions = []\n",
    "    \n",
    "    # Get all unique class ids across all predictions\n",
    "    all_classes = set()\n",
    "    for preds in pred_tensors:\n",
    "        if len(preds) > 0:\n",
    "            all_classes.update(preds[:, 0].int().tolist())\n",
    "    \n",
    "    for class_id in all_classes:\n",
    "        # Extract predictions for this class from each model\n",
    "        class_preds = []\n",
    "        class_weights = []\n",
    "        \n",
    "        for i, preds in enumerate(pred_tensors):\n",
    "            if len(preds) > 0:\n",
    "                class_mask = preds[:, 0] == class_id\n",
    "                if torch.any(class_mask):\n",
    "                    class_preds.append(preds[class_mask])\n",
    "                    class_weights.append(filtered_weights[i])\n",
    "        \n",
    "        if not class_preds:\n",
    "            continue\n",
    "        \n",
    "        # Combine all predictions for this class\n",
    "        all_boxes = torch.cat([p[:, 1:5] for p in class_preds])\n",
    "        all_scores = torch.cat([p[:, 5] * w for p, w in zip(class_preds, class_weights)])\n",
    "        all_labels = torch.ones(len(all_boxes)) * class_id\n",
    "        \n",
    "        # Group boxes by IoU\n",
    "        clusters = []\n",
    "        cluster_scores = []\n",
    "        used_indices = set()\n",
    "        \n",
    "        for i in range(len(all_boxes)):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "                \n",
    "            # Start a new cluster\n",
    "            cluster_boxes = [all_boxes[i]]\n",
    "            cluster_s = [all_scores[i]]\n",
    "            used_indices.add(i)\n",
    "            \n",
    "            # Find all overlapping boxes\n",
    "            for j in range(i+1, len(all_boxes)):\n",
    "                if j in used_indices:\n",
    "                    continue\n",
    "                    \n",
    "                box1 = all_boxes[i]\n",
    "                box2 = all_boxes[j]\n",
    "                \n",
    "                # Calculate IoU\n",
    "                x1 = max(box1[0], box2[0])\n",
    "                y1 = max(box1[1], box2[1])\n",
    "                x2 = min(box1[2], box2[2])\n",
    "                y2 = min(box1[3], box2[3])\n",
    "                \n",
    "                if x2 < x1 or y2 < y1:\n",
    "                    iou = 0.0\n",
    "                else:\n",
    "                    intersection = (x2 - x1) * (y2 - y1)\n",
    "                    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "                    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "                    iou = intersection / (area1 + area2 - intersection)\n",
    "                \n",
    "                if iou >= iou_thresh:\n",
    "                    cluster_boxes.append(all_boxes[j])\n",
    "                    cluster_s.append(all_scores[j])\n",
    "                    used_indices.add(j)\n",
    "            \n",
    "            # Average the boxes in the cluster, weighted by confidence\n",
    "            if len(cluster_boxes) > 0:\n",
    "                cluster_boxes = torch.stack(cluster_boxes)\n",
    "                cluster_s = torch.stack(cluster_s)\n",
    "                \n",
    "                # Apply weights based on confidence\n",
    "                weights = cluster_s / cluster_s.sum()\n",
    "                weights = weights.unsqueeze(1).repeat(1, 4)\n",
    "                \n",
    "                # Calculate weighted average box\n",
    "                fused_box = (cluster_boxes * weights).sum(dim=0)\n",
    "                fused_score = cluster_s.mean()  # Average score\n",
    "                \n",
    "                clusters.append(fused_box)\n",
    "                cluster_scores.append(fused_score)\n",
    "        \n",
    "        # Create final predictions for this class\n",
    "        for box, score in zip(clusters, cluster_scores):\n",
    "            final_predictions.append(torch.cat([torch.tensor([class_id]), box, torch.tensor([score])]))\n",
    "    \n",
    "    if final_predictions:\n",
    "        return torch.stack(final_predictions).numpy()\n",
    "    else:\n",
    "        return np.empty((0, 6))\n",
    "\n",
    "def soft_weighted_nms(predictions, iou_thresh=0.5, sigma=0.5, score_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Apply Soft-NMS to predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predictions [class_id, x1, y1, x2, y2, conf]\n",
    "    - iou_thresh: IoU threshold for NMS\n",
    "    - sigma: Parameter for Gaussian penalty function\n",
    "    - score_threshold: Minimum score threshold to keep a box\n",
    "    \n",
    "    Returns:\n",
    "    - Filtered predictions\n",
    "    \"\"\"\n",
    "    if len(predictions) == 0:\n",
    "        return np.empty((0, 6))\n",
    "    \n",
    "    # Group by class\n",
    "    class_groups = defaultdict(list)\n",
    "    for pred in predictions:\n",
    "        class_groups[int(pred[0])].append(pred)\n",
    "    \n",
    "    final_predictions = []\n",
    "    \n",
    "    for class_id, preds in class_groups.items():\n",
    "        preds = np.array(preds)\n",
    "        if len(preds) == 1:\n",
    "            final_predictions.append(preds[0])\n",
    "            continue\n",
    "            \n",
    "        # Sort by confidence score\n",
    "        order = np.argsort(-preds[:, 5])\n",
    "        preds = preds[order]\n",
    "        \n",
    "        boxes = preds[:, 1:5]\n",
    "        scores = preds[:, 5].copy()\n",
    "        \n",
    "        for i in range(len(boxes)):\n",
    "            if scores[i] < score_threshold:\n",
    "                continue\n",
    "                \n",
    "            # Keep the current box\n",
    "            box_i = boxes[i]\n",
    "            \n",
    "            # Update scores of all other boxes\n",
    "            for j in range(i+1, len(boxes)):\n",
    "                if scores[j] < score_threshold:\n",
    "                    continue\n",
    "                    \n",
    "                box_j = boxes[j]\n",
    "                \n",
    "                # Calculate IoU\n",
    "                xx1 = max(box_i[0], box_j[0])\n",
    "                yy1 = max(box_i[1], box_j[1])\n",
    "                xx2 = min(box_i[2], box_j[2])\n",
    "                yy2 = min(box_i[3], box_j[3])\n",
    "                \n",
    "                w = max(0, xx2 - xx1)\n",
    "                h = max(0, yy2 - yy1)\n",
    "                \n",
    "                intersection = w * h\n",
    "                area_i = (box_i[2] - box_i[0]) * (box_i[3] - box_i[1])\n",
    "                area_j = (box_j[2] - box_j[0]) * (box_j[3] - box_j[1])\n",
    "                union = area_i + area_j - intersection\n",
    "                \n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                \n",
    "                # Apply Gaussian penalty to overlapping boxes\n",
    "                if iou > iou_thresh:\n",
    "                    scores[j] *= np.exp(-(iou * iou) / sigma)\n",
    "        \n",
    "        # Add boxes that are still above the threshold\n",
    "        for i in range(len(preds)):\n",
    "            if scores[i] >= score_threshold:\n",
    "                pred_i = preds[i].copy()\n",
    "                pred_i[5] = scores[i]  # Update with new score\n",
    "                final_predictions.append(pred_i)\n",
    "    \n",
    "    if final_predictions:\n",
    "        return np.array(final_predictions)\n",
    "    else:\n",
    "        return np.empty((0, 6))\n",
    "\n",
    "def advanced_ensemble(yolo_folder, kdvit_folder, save_folder, \n",
    "                     iou_thresh=0.5, conf_thresh=0.1, \n",
    "                     model_weights=None, use_wbf=True):\n",
    "    \"\"\"\n",
    "    Advanced ensemble combining YOLOv9 and KD-YOLOX-ViT predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - yolo_folder: Directory containing YOLOv9 predictions\n",
    "    - kdvit_folder: Directory containing KD-YOLOX-ViT predictions\n",
    "    - save_folder: Directory to save ensemble results\n",
    "    - iou_thresh: IoU threshold for fusion\n",
    "    - conf_thresh: Confidence threshold for filtering weak predictions\n",
    "    - model_weights: List of weights for each model [yolo_weight, kdvit_weight]\n",
    "    - use_wbf: If True, use Weighted Box Fusion, otherwise use Soft-NMS\n",
    "    \"\"\"\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    # Default weights favor the better model (YOLOv9)\n",
    "    if model_weights is None:\n",
    "        model_weights = [0.7, 0.3]  # YOLOv9 has higher weight\n",
    "    \n",
    "    # Get files from yolo folder\n",
    "    files = [f for f in os.listdir(yolo_folder) if f.endswith('.txt')]\n",
    "    \n",
    "    for file in files:\n",
    "        yolov9_path = os.path.join(yolo_folder, file)\n",
    "        kdvit_path = os.path.join(kdvit_folder, file)\n",
    "        save_path = os.path.join(save_folder, file)\n",
    "        \n",
    "        # Skip if kdvit prediction doesn't exist\n",
    "        if not os.path.exists(kdvit_path):\n",
    "            print(f\"âš ï¸ Missing KD-ViT prediction for {file}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load predictions\n",
    "            yolov9_preds = np.loadtxt(yolov9_path).reshape(-1, 6) if os.path.getsize(yolov9_path) > 0 else np.empty((0, 6))\n",
    "            kdvit_preds = np.loadtxt(kdvit_path).reshape(-1, 6) if os.path.getsize(kdvit_path) > 0 else np.empty((0, 6))\n",
    "            \n",
    "            # Handle empty files or single detection\n",
    "            if yolov9_preds.size > 0 and yolov9_preds.ndim == 1:\n",
    "                yolov9_preds = yolov9_preds.reshape(1, -1)\n",
    "            if kdvit_preds.size > 0 and kdvit_preds.ndim == 1:\n",
    "                kdvit_preds = kdvit_preds.reshape(1, -1)\n",
    "                \n",
    "            # Class-specific weighting for YOLOv9\n",
    "            # YOLOv9 performs better on most classes, but KD-ViT is better on epithn and leuko\n",
    "            for i in range(len(yolov9_preds)):\n",
    "                class_id = int(yolov9_preds[i, 0])\n",
    "                # Boost 'cast' and 'mycete' classes where YOLOv9 is significantly better\n",
    "                if class_id == 0:  # cast\n",
    "                    yolov9_preds[i, 5] *= 1.1\n",
    "                elif class_id == 6:  # mycete\n",
    "                    yolov9_preds[i, 5] *= 1.05\n",
    "            \n",
    "            # Boost certain classes for KD-ViT where it performs better\n",
    "            for i in range(len(kdvit_preds)):\n",
    "                class_id = int(kdvit_preds[i, 0])\n",
    "                # Boost 'leuko' class where KD-ViT is better\n",
    "                if class_id == 5:  # leuko\n",
    "                    kdvit_preds[i, 5] *= 1.1\n",
    "            \n",
    "            # Apply fusion method\n",
    "            if use_wbf:\n",
    "                ensemble_preds = weighted_box_fusion(\n",
    "                    [yolov9_preds, kdvit_preds],\n",
    "                    model_weights,\n",
    "                    iou_thresh,\n",
    "                    conf_thresh\n",
    "                )\n",
    "            else:\n",
    "                # Combine with confidence-weighted approach\n",
    "                if len(yolov9_preds) == 0 and len(kdvit_preds) == 0:\n",
    "                    ensemble_preds = np.empty((0, 6))\n",
    "                else:\n",
    "                    # Apply model-specific weights to confidence scores\n",
    "                    if len(yolov9_preds) > 0:\n",
    "                        yolov9_preds[:, 5] *= model_weights[0]\n",
    "                    if len(kdvit_preds) > 0:\n",
    "                        kdvit_preds[:, 5] *= model_weights[1]\n",
    "                    \n",
    "                    # Combine predictions\n",
    "                    combined_preds = np.vstack((yolov9_preds, kdvit_preds)) if len(yolov9_preds) > 0 and len(kdvit_preds) > 0 else (\n",
    "                        yolov9_preds if len(yolov9_preds) > 0 else kdvit_preds\n",
    "                    )\n",
    "                    \n",
    "                    # Apply Soft-NMS\n",
    "                    ensemble_preds = soft_weighted_nms(combined_preds, iou_thresh)\n",
    "            \n",
    "            # Save results\n",
    "            np.savetxt(save_path, ensemble_preds, fmt='%.6f')\n",
    "            #print(f\"âœ… Advanced ensemble saved: {file} | Detections: {len(ensemble_preds)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error processing {file}: {str(e)}\")\n",
    "\n",
    "# === Configuration ===\n",
    "# Folders\n",
    "model1_dir = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/yolov9_corners'\n",
    "model2_dir = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/kdvit_corners'\n",
    "ensemble_dir = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/advanced_ensemble_output'\n",
    "\n",
    "# Model weights - give more weight to YOLOv9 as it performs better overall\n",
    "model_weights = [0.7, 0.3]  # [YOLOv9, KD-ViT]\n",
    "\n",
    "# === Run Advanced Ensemble ===\n",
    "advanced_ensemble(\n",
    "    model1_dir, \n",
    "    model2_dir, \n",
    "    ensemble_dir,\n",
    "    iou_thresh=0.5, \n",
    "    conf_thresh=0.1,\n",
    "    model_weights=model_weights,\n",
    "    use_wbf=True  # Set to True for Weighted Box Fusion, False for Soft-NMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681f1df",
   "metadata": {},
   "source": [
    "mAP calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685db625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Average Precision:\n",
      "--------------------------------------------------\n",
      "epith      - AP: 0.9441, GT count: 412\n",
      "rbc/wbc    - AP: 0.9487, GT count: 2038\n",
      "--------------------------------------------------\n",
      "Final mAP@50: 0.9464\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Only allow these two classes\n",
    "valid_class_ids = {0, 1}\n",
    "class_names = {0: \"epith\", 1: \"rbc/wbc\"}\n",
    "\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1_min = box1[0] - box1[2] / 2\n",
    "    x1_max = box1[0] + box1[2] / 2\n",
    "    y1_min = box1[1] - box1[3] / 2\n",
    "    y1_max = box1[1] + box1[3] / 2\n",
    "\n",
    "    x2_min = box2[0] - box2[2] / 2\n",
    "    x2_max = box2[0] + box2[2] / 2\n",
    "    y2_min = box2[1] - box2[3] / 2\n",
    "    y2_max = box2[1] + box2[3] / 2\n",
    "\n",
    "    inter_xmin = max(x1_min, x2_min)\n",
    "    inter_ymin = max(y1_min, y2_min)\n",
    "    inter_xmax = min(x1_max, x2_max)\n",
    "    inter_ymax = min(y1_max, y2_max)\n",
    "\n",
    "    inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "\n",
    "def evaluate_map50_coco(pred_folder, gt_folder, iou_thresh=0.5, conf_thresh=0.001):\n",
    "    all_predictions = defaultdict(list)\n",
    "    gt_counter_per_class = defaultdict(int)\n",
    "\n",
    "    files = [f for f in os.listdir(pred_folder) if f.endswith('.txt') and os.path.exists(os.path.join(gt_folder, f))]\n",
    "\n",
    "    for file in files:\n",
    "        gt_path = os.path.join(gt_folder, file)\n",
    "        pred_path = os.path.join(pred_folder, file)\n",
    "\n",
    "        gt_data = np.loadtxt(gt_path, ndmin=2) if os.path.getsize(gt_path) > 0 else np.empty((0, 5))\n",
    "        pred_data = np.loadtxt(pred_path, ndmin=2) if os.path.getsize(pred_path) > 0 else np.empty((0, 6))\n",
    "\n",
    "        gt_this_image = {}\n",
    "        for gt_box in gt_data:\n",
    "            class_id = int(gt_box[0])\n",
    "            if class_id not in valid_class_ids:\n",
    "                continue\n",
    "            gt_counter_per_class[class_id] += 1\n",
    "            gt_this_image.setdefault(class_id, []).append([False, *gt_box[1:]])\n",
    "\n",
    "        for pred_box in pred_data:\n",
    "            class_id = int(pred_box[0])\n",
    "            confidence = float(pred_box[5])\n",
    "            if confidence < conf_thresh or class_id not in valid_class_ids:\n",
    "                continue\n",
    "            pred_bbox = [float(x) for x in pred_box[1:5]]\n",
    "            all_predictions[class_id].append([file, confidence] + pred_bbox + [False])\n",
    "\n",
    "            if class_id in gt_this_image:\n",
    "                max_iou = -1\n",
    "                max_idx = -1\n",
    "                for idx, gt_box in enumerate(gt_this_image[class_id]):\n",
    "                    if gt_box[0]:\n",
    "                        continue\n",
    "                    iou = calculate_iou(pred_bbox, gt_box[1:])\n",
    "                    if iou > max_iou:\n",
    "                        max_iou = iou\n",
    "                        max_idx = idx\n",
    "                if max_iou >= iou_thresh and max_idx >= 0:\n",
    "                    gt_this_image[class_id][max_idx][0] = True\n",
    "                    all_predictions[class_id][-1][-1] = True\n",
    "\n",
    "    sum_ap = 0\n",
    "    ap_dictionary = {}\n",
    "    valid_classes = 0\n",
    "\n",
    "    print(\"\\nClass-wise Average Precision:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for class_id in sorted(valid_class_ids):\n",
    "        if gt_counter_per_class[class_id] == 0:\n",
    "            continue\n",
    "\n",
    "        predictions = all_predictions.get(class_id, [])\n",
    "        if not predictions:\n",
    "            ap_dictionary[class_id] = 0.0\n",
    "            sum_ap += 0.0\n",
    "            valid_classes += 1\n",
    "            print(f\"{class_names[class_id]:<10} - AP: 0.0000, GT count: {gt_counter_per_class[class_id]}\")\n",
    "            continue\n",
    "\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        tp = np.array([p[-1] for p in predictions], dtype=np.float64)\n",
    "        fp = np.logical_not(tp).astype(np.float64)\n",
    "\n",
    "        cumsum_tp = np.cumsum(tp)\n",
    "        cumsum_fp = np.cumsum(fp)\n",
    "\n",
    "        precision = cumsum_tp / (cumsum_tp + cumsum_fp + 1e-10)\n",
    "        recall = cumsum_tp / gt_counter_per_class[class_id]\n",
    "\n",
    "        for i in range(len(precision) - 2, -1, -1):\n",
    "            precision[i] = max(precision[i], precision[i + 1])\n",
    "\n",
    "        recall_points = np.unique(np.concatenate(([0], recall, [1])))\n",
    "        interpolated_precision = np.zeros_like(recall_points)\n",
    "\n",
    "        for i, r in enumerate(recall_points):\n",
    "            precisions_at_recall = precision[recall >= r]\n",
    "            if len(precisions_at_recall):\n",
    "                interpolated_precision[i] = np.max(precisions_at_recall)\n",
    "\n",
    "        ap = np.sum((recall_points[1:] - recall_points[:-1]) * interpolated_precision[:-1])\n",
    "        ap_dictionary[class_id] = ap\n",
    "        sum_ap += ap\n",
    "        valid_classes += 1\n",
    "\n",
    "        print(f\"{class_names[class_id]:<10} - AP: {ap:.4f}, GT count: {gt_counter_per_class[class_id]}\")\n",
    "\n",
    "    mAP = sum_ap / valid_classes if valid_classes else 0.0\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Final mAP@50: {mAP:.4f}\")\n",
    "    return mAP, all_predictions, gt_counter_per_class\n",
    "\n",
    "\n",
    "def plot_precision_recall_curves(all_predictions, gt_counter_per_class):\n",
    "    for class_id in sorted(valid_class_ids):\n",
    "        predictions = all_predictions.get(class_id, [])\n",
    "        if not predictions or gt_counter_per_class[class_id] == 0:\n",
    "            continue\n",
    "\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        tp = np.array([p[-1] for p in predictions], dtype=np.float64)\n",
    "        fp = np.logical_not(tp).astype(np.float64)\n",
    "\n",
    "        cumsum_tp = np.cumsum(tp)\n",
    "        cumsum_fp = np.cumsum(fp)\n",
    "\n",
    "        precision = cumsum_tp / (cumsum_tp + cumsum_fp + 1e-10)\n",
    "        recall = cumsum_tp / gt_counter_per_class[class_id]\n",
    "\n",
    "        plt.plot(recall, precision, marker='.', label=class_names[class_id])\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gt_folder = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/gt_corners'\n",
    "    pred_folder = 'C:/Mansura/UTI-Revision2/ExternalValidation/NMS/advanced_ensemble_output'\n",
    "    \n",
    "    # Calculate mAP@50 using COCO method with same confidence threshold as YOLO (0.001)\n",
    "    map50 = evaluate_map50_coco(pred_folder, gt_folder, iou_thresh=0.5, conf_thresh=0.001)\n",
    "    \n",
    "    # Optionally plot precision-recall curves\n",
    "    #plot_precision_recall_curves(pred_folder, gt_folder, iou_thresh=0.5, conf_thresh=0.001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
