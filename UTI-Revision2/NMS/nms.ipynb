{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb08f6d0",
   "metadata": {},
   "source": [
    "Here's a Python script to:\n",
    "\n",
    "Load prediction files from both models.\n",
    "\n",
    "Load ground truth.\n",
    "\n",
    "Compare their formats (number of columns, data types, column meanings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e46867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading C:/Mansura/UTI-Revision2/NMS/yolov8x_predictions/nh00010.txt: C:/Mansura/UTI-Revision2/NMS/yolov8x_predictions/nh00010.txt not found.\n",
      "========== FORMAT CHECK ==========\n",
      "\n",
      "YOLOv9 Prediction shape: (39, 6)\n",
      "Sample row from YOLOv9 Prediction: [6.        0.711089  0.545911  0.038479  0.0845022 0.852539 ]\n",
      "🔍 YOLOv9 Prediction appears to be a **prediction** with 6 columns:\n",
      "   Likely format: [class_id, x_center, y_center, width, height, confidence]\n",
      "\n",
      "KD-YOLO-ViT Prediction: No data found or file empty.\n",
      "Ground Truth shape: (11, 5)\n",
      "Sample row from Ground Truth: [5.         0.27625    0.13333333 0.035      0.06      ]\n",
      "📌 Ground Truth appears to be a **ground truth** with 5 columns:\n",
      "   Likely format: [class_id, x_center, y_center, width, height]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_predictions(file_path):\n",
    "    try:\n",
    "        data = np.loadtxt(file_path)\n",
    "        if data.ndim == 1:\n",
    "            data = data.reshape(1, -1)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def describe_format(name, data):\n",
    "    if data is None or data.shape[0] == 0:\n",
    "        print(f\"{name}: No data found or file empty.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"{name} shape: {data.shape}\")\n",
    "    sample = data[0]\n",
    "    print(f\"Sample row from {name}: {sample}\")\n",
    "\n",
    "    num_columns = data.shape[1]\n",
    "\n",
    "    if num_columns == 6:\n",
    "        print(f\"🔍 {name} appears to be a **prediction** with 6 columns:\")\n",
    "        print(\"   Likely format: [class_id, x_center, y_center, width, height, confidence]\")\n",
    "    elif num_columns == 5:\n",
    "        print(f\"📌 {name} appears to be a **ground truth** with 5 columns:\")\n",
    "        print(\"   Likely format: [class_id, x_center, y_center, width, height]\")\n",
    "    else:\n",
    "        print(f\"⚠️ {name} has {num_columns} columns, format unknown. Please inspect manually.\")\n",
    "\n",
    "    print()\n",
    "\n",
    "def check_all_formats(yolo9_pred, kdvit_pred, gt):\n",
    "    print(\"========== FORMAT CHECK ==========\\n\")\n",
    "    describe_format(\"YOLOv9 Prediction\", yolo9_pred)\n",
    "    describe_format(\"KD-YOLO-ViT Prediction\", kdvit_pred)\n",
    "    describe_format(\"Ground Truth\", gt)\n",
    "    \n",
    "    # Check if predictions can be ensembled directly\n",
    "    if yolo9_pred is not None and kdvit_pred is not None:\n",
    "        if yolo9_pred.shape[1] != kdvit_pred.shape[1]:\n",
    "            print(\"❌ Prediction formats mismatch! Cannot ensemble directly.\")\n",
    "        else:\n",
    "            print(\"✅ Prediction formats match. You can proceed to ensemble.\")\n",
    "\n",
    "# ======= Example usage =======\n",
    "yolov9_pred_file = 'C:/Mansura/UTI-Revision2/NMS/yolov9e_predictions/nh00010.txt'\n",
    "kd_yolo_vit_pred_file = 'C:/Mansura/UTI-Revision2/NMS/yolov8x_predictions/nh00010.txt'\n",
    "ground_truth_file = 'C:/Mansura/UTI-Revision2/NMS/test_labels/nh00010.txt'\n",
    "\n",
    "yolov9_preds = load_predictions(yolov9_pred_file)\n",
    "kd_yolo_vit_preds = load_predictions(kd_yolo_vit_pred_file)\n",
    "ground_truth = load_predictions(ground_truth_file)\n",
    "\n",
    "check_all_formats(yolov9_preds, kd_yolo_vit_preds, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4672deb",
   "metadata": {},
   "source": [
    "🔁 You currently have: [class_id, x_center, y_center, width, height, confidence]\n",
    "For NMS convert: [x1, y1, x2, y2, confidence, class_id]\n",
    "\n",
    "x1 = x_center - width / 2\n",
    "\n",
    "y1 = y_center - height / 2\n",
    "\n",
    "x2 = x_center + width / 2\n",
    "\n",
    "y2 = y_center + height / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68aa578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_corners(data, is_prediction=True):\n",
    "    \"\"\"Convert [class_id, cx, cy, w, h, (conf)] -> [class_id, x1, y1, x2, y2, (conf)]\"\"\"\n",
    "    converted = []\n",
    "    for row in data:\n",
    "        class_id, cx, cy, w, h = row[:5]\n",
    "        x1 = cx - w / 2\n",
    "        y1 = cy - h / 2\n",
    "        x2 = cx + w / 2\n",
    "        y2 = cy + h / 2\n",
    "        if is_prediction:\n",
    "            conf = row[5]\n",
    "            converted.append([class_id, x1, y1, x2, y2, conf])\n",
    "        else:\n",
    "            converted.append([class_id, x1, y1, x2, y2])\n",
    "    return np.array(converted)\n",
    "\n",
    "def process_folder(input_folder, output_folder, is_prediction=True):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for file in os.listdir(input_folder):\n",
    "        if not file.endswith('.txt'):\n",
    "            continue\n",
    "        input_path = os.path.join(input_folder, file)\n",
    "        output_path = os.path.join(output_folder, file)\n",
    "\n",
    "        try:\n",
    "            data = np.loadtxt(input_path)\n",
    "            if data.ndim == 1:\n",
    "                data = data.reshape(1, -1)\n",
    "            converted = convert_to_corners(data, is_prediction=is_prediction)\n",
    "            np.savetxt(output_path, converted, fmt='%.6f')\n",
    "            #print(f\"✅ Converted and saved: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")\n",
    "\n",
    "# === File paths ===\n",
    "yolov9_input = 'C:/Mansura/UTI-Revision2/NMS/yolov9e_predictions'\n",
    "yolov8_input = 'C:/Mansura/UTI-Revision2/NMS/yolov8x_predictions'\n",
    "yolov10_input = 'C:/Mansura/UTI-Revision2/NMS/yolov10x_predictions'\n",
    "kdvit_input = 'C:/Mansura/UTI-Revision2/NMS/kd-yolox-vit_predictions'\n",
    "gt_input = 'C:/Mansura/UTI-Revision2/NMS/test_labels'\n",
    "\n",
    "yolov9_output = 'C:/Mansura/UTI-Revision2/NMS/yolov9_corners'\n",
    "yolov8_output = 'C:/Mansura/UTI-Revision2/NMS/yolov8_corners'\n",
    "yolov10_output = 'C:/Mansura/UTI-Revision2/NMS/yolov10_corners'\n",
    "kdvit_output = 'C:/Mansura/UTI-Revision2/NMS/kdvit_corners'\n",
    "gt_output = 'C:/Mansura/UTI-Revision2/NMS/gt_corners'\n",
    "\n",
    "# === Run Conversion ===\n",
    "process_folder(yolov9_input, yolov9_output, is_prediction=True)\n",
    "process_folder(yolov8_input, yolov8_output, is_prediction=True)\n",
    "process_folder(yolov10_input, yolov10_output, is_prediction=True)\n",
    "process_folder(kdvit_input, kdvit_output, is_prediction=True)\n",
    "process_folder(gt_input, gt_output, is_prediction=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba1af8",
   "metadata": {},
   "source": [
    "Improved ensemble by nms (wbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bc4179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from collections import defaultdict\n",
    "\n",
    "def weighted_box_fusion(predictions_list, model_weights, iou_thresh=0.5, conf_thresh=0.1):\n",
    "    \"\"\"\n",
    "    Apply Weighted Box Fusion (WBF) to combine predictions from multiple models\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions_list: List of numpy arrays, each containing predictions from one model\n",
    "                       Each prediction has format [class_id, x1, y1, x2, y2, conf]\n",
    "    - model_weights: List of weights for each model\n",
    "    - iou_thresh: IoU threshold for clustering boxes\n",
    "    - conf_thresh: Confidence threshold for filtering weak predictions\n",
    "    \n",
    "    Returns:\n",
    "    - Array of fused predictions [class_id, x1, y1, x2, y2, conf]\n",
    "    \"\"\"\n",
    "    # Filter empty predictions and apply confidence threshold\n",
    "    filtered_preds = []\n",
    "    filtered_weights = []\n",
    "    \n",
    "    for i, preds in enumerate(predictions_list):\n",
    "        if len(preds) > 0:\n",
    "            mask = preds[:, 5] >= conf_thresh\n",
    "            if np.any(mask):\n",
    "                filtered_preds.append(preds[mask])\n",
    "                filtered_weights.append(model_weights[i])\n",
    "    \n",
    "    if not filtered_preds:\n",
    "        return np.empty((0, 6))\n",
    "    \n",
    "    # Convert all to tensors\n",
    "    pred_tensors = [torch.tensor(p, dtype=torch.float32) for p in filtered_preds]\n",
    "    \n",
    "    # Process each class separately\n",
    "    final_predictions = []\n",
    "    \n",
    "    # Get all unique class ids across all predictions\n",
    "    all_classes = set()\n",
    "    for preds in pred_tensors:\n",
    "        if len(preds) > 0:\n",
    "            all_classes.update(preds[:, 0].int().tolist())\n",
    "    \n",
    "    for class_id in all_classes:\n",
    "        # Extract predictions for this class from each model\n",
    "        class_preds = []\n",
    "        class_weights = []\n",
    "        \n",
    "        for i, preds in enumerate(pred_tensors):\n",
    "            if len(preds) > 0:\n",
    "                class_mask = preds[:, 0] == class_id\n",
    "                if torch.any(class_mask):\n",
    "                    class_preds.append(preds[class_mask])\n",
    "                    class_weights.append(filtered_weights[i])\n",
    "        \n",
    "        if not class_preds:\n",
    "            continue\n",
    "        \n",
    "        # Combine all predictions for this class\n",
    "        all_boxes = torch.cat([p[:, 1:5] for p in class_preds])\n",
    "        all_scores = torch.cat([p[:, 5] * w for p, w in zip(class_preds, class_weights)])\n",
    "        all_labels = torch.ones(len(all_boxes)) * class_id\n",
    "        \n",
    "        # Group boxes by IoU\n",
    "        clusters = []\n",
    "        cluster_scores = []\n",
    "        used_indices = set()\n",
    "        \n",
    "        for i in range(len(all_boxes)):\n",
    "            if i in used_indices:\n",
    "                continue\n",
    "                \n",
    "            # Start a new cluster\n",
    "            cluster_boxes = [all_boxes[i]]\n",
    "            cluster_s = [all_scores[i]]\n",
    "            used_indices.add(i)\n",
    "            \n",
    "            # Find all overlapping boxes\n",
    "            for j in range(i+1, len(all_boxes)):\n",
    "                if j in used_indices:\n",
    "                    continue\n",
    "                    \n",
    "                box1 = all_boxes[i]\n",
    "                box2 = all_boxes[j]\n",
    "                \n",
    "                # Calculate IoU\n",
    "                x1 = max(box1[0], box2[0])\n",
    "                y1 = max(box1[1], box2[1])\n",
    "                x2 = min(box1[2], box2[2])\n",
    "                y2 = min(box1[3], box2[3])\n",
    "                \n",
    "                if x2 < x1 or y2 < y1:\n",
    "                    iou = 0.0\n",
    "                else:\n",
    "                    intersection = (x2 - x1) * (y2 - y1)\n",
    "                    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "                    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "                    iou = intersection / (area1 + area2 - intersection)\n",
    "                \n",
    "                if iou >= iou_thresh:\n",
    "                    cluster_boxes.append(all_boxes[j])\n",
    "                    cluster_s.append(all_scores[j])\n",
    "                    used_indices.add(j)\n",
    "            \n",
    "            # Average the boxes in the cluster, weighted by confidence\n",
    "            if len(cluster_boxes) > 0:\n",
    "                cluster_boxes = torch.stack(cluster_boxes)\n",
    "                cluster_s = torch.stack(cluster_s)\n",
    "                \n",
    "                # Apply weights based on confidence\n",
    "                weights = cluster_s / cluster_s.sum()\n",
    "                weights = weights.unsqueeze(1).repeat(1, 4)\n",
    "                \n",
    "                # Calculate weighted average box\n",
    "                fused_box = (cluster_boxes * weights).sum(dim=0)\n",
    "                fused_score = cluster_s.mean()  # Average score\n",
    "                \n",
    "                clusters.append(fused_box)\n",
    "                cluster_scores.append(fused_score)\n",
    "        \n",
    "        # Create final predictions for this class\n",
    "        for box, score in zip(clusters, cluster_scores):\n",
    "            final_predictions.append(torch.cat([torch.tensor([class_id]), box, torch.tensor([score])]))\n",
    "    \n",
    "    if final_predictions:\n",
    "        return torch.stack(final_predictions).numpy()\n",
    "    else:\n",
    "        return np.empty((0, 6))\n",
    "\n",
    "def soft_weighted_nms(predictions, iou_thresh=0.5, sigma=0.5, score_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Apply Soft-NMS to predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predictions [class_id, x1, y1, x2, y2, conf]\n",
    "    - iou_thresh: IoU threshold for NMS\n",
    "    - sigma: Parameter for Gaussian penalty function\n",
    "    - score_threshold: Minimum score threshold to keep a box\n",
    "    \n",
    "    Returns:\n",
    "    - Filtered predictions\n",
    "    \"\"\"\n",
    "    if len(predictions) == 0:\n",
    "        return np.empty((0, 6))\n",
    "    \n",
    "    # Group by class\n",
    "    class_groups = defaultdict(list)\n",
    "    for pred in predictions:\n",
    "        class_groups[int(pred[0])].append(pred)\n",
    "    \n",
    "    final_predictions = []\n",
    "    \n",
    "    for class_id, preds in class_groups.items():\n",
    "        preds = np.array(preds)\n",
    "        if len(preds) == 1:\n",
    "            final_predictions.append(preds[0])\n",
    "            continue\n",
    "            \n",
    "        # Sort by confidence score\n",
    "        order = np.argsort(-preds[:, 5])\n",
    "        preds = preds[order]\n",
    "        \n",
    "        boxes = preds[:, 1:5]\n",
    "        scores = preds[:, 5].copy()\n",
    "        \n",
    "        for i in range(len(boxes)):\n",
    "            if scores[i] < score_threshold:\n",
    "                continue\n",
    "                \n",
    "            # Keep the current box\n",
    "            box_i = boxes[i]\n",
    "            \n",
    "            # Update scores of all other boxes\n",
    "            for j in range(i+1, len(boxes)):\n",
    "                if scores[j] < score_threshold:\n",
    "                    continue\n",
    "                    \n",
    "                box_j = boxes[j]\n",
    "                \n",
    "                # Calculate IoU\n",
    "                xx1 = max(box_i[0], box_j[0])\n",
    "                yy1 = max(box_i[1], box_j[1])\n",
    "                xx2 = min(box_i[2], box_j[2])\n",
    "                yy2 = min(box_i[3], box_j[3])\n",
    "                \n",
    "                w = max(0, xx2 - xx1)\n",
    "                h = max(0, yy2 - yy1)\n",
    "                \n",
    "                intersection = w * h\n",
    "                area_i = (box_i[2] - box_i[0]) * (box_i[3] - box_i[1])\n",
    "                area_j = (box_j[2] - box_j[0]) * (box_j[3] - box_j[1])\n",
    "                union = area_i + area_j - intersection\n",
    "                \n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                \n",
    "                # Apply Gaussian penalty to overlapping boxes\n",
    "                if iou > iou_thresh:\n",
    "                    scores[j] *= np.exp(-(iou * iou) / sigma)\n",
    "        \n",
    "        # Add boxes that are still above the threshold\n",
    "        for i in range(len(preds)):\n",
    "            if scores[i] >= score_threshold:\n",
    "                pred_i = preds[i].copy()\n",
    "                pred_i[5] = scores[i]  # Update with new score\n",
    "                final_predictions.append(pred_i)\n",
    "    \n",
    "    if final_predictions:\n",
    "        return np.array(final_predictions)\n",
    "    else:\n",
    "        return np.empty((0, 6))\n",
    "\n",
    "def advanced_ensemble(yolo_folder, kdvit_folder, save_folder, \n",
    "                     iou_thresh=0.5, conf_thresh=0.1, \n",
    "                     model_weights=None, use_wbf=True):\n",
    "    \"\"\"\n",
    "    Advanced ensemble combining YOLOv9 and KD-YOLOX-ViT predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - yolo_folder: Directory containing YOLOv9 predictions\n",
    "    - kdvit_folder: Directory containing KD-YOLOX-ViT predictions\n",
    "    - save_folder: Directory to save ensemble results\n",
    "    - iou_thresh: IoU threshold for fusion\n",
    "    - conf_thresh: Confidence threshold for filtering weak predictions\n",
    "    - model_weights: List of weights for each model [yolo_weight, kdvit_weight]\n",
    "    - use_wbf: If True, use Weighted Box Fusion, otherwise use Soft-NMS\n",
    "    \"\"\"\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    # Default weights favor the better model (YOLOv9)\n",
    "    if model_weights is None:\n",
    "        model_weights = [0.7, 0.3]  # YOLOv9 has higher weight\n",
    "    \n",
    "    # Get files from yolo folder\n",
    "    files = [f for f in os.listdir(yolo_folder) if f.endswith('.txt')]\n",
    "    \n",
    "    for file in files:\n",
    "        yolov9_path = os.path.join(yolo_folder, file)\n",
    "        kdvit_path = os.path.join(kdvit_folder, file)\n",
    "        save_path = os.path.join(save_folder, file)\n",
    "        \n",
    "        # Skip if kdvit prediction doesn't exist\n",
    "        if not os.path.exists(kdvit_path):\n",
    "            print(f\"⚠️ Missing KD-ViT prediction for {file}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load predictions\n",
    "            yolov9_preds = np.loadtxt(yolov9_path).reshape(-1, 6) if os.path.getsize(yolov9_path) > 0 else np.empty((0, 6))\n",
    "            kdvit_preds = np.loadtxt(kdvit_path).reshape(-1, 6) if os.path.getsize(kdvit_path) > 0 else np.empty((0, 6))\n",
    "            \n",
    "            # Handle empty files or single detection\n",
    "            if yolov9_preds.size > 0 and yolov9_preds.ndim == 1:\n",
    "                yolov9_preds = yolov9_preds.reshape(1, -1)\n",
    "            if kdvit_preds.size > 0 and kdvit_preds.ndim == 1:\n",
    "                kdvit_preds = kdvit_preds.reshape(1, -1)\n",
    "                \n",
    "            # Class-specific weighting for YOLOv9\n",
    "            # YOLOv9 performs better on most classes, but KD-ViT is better on epithn and leuko\n",
    "            for i in range(len(yolov9_preds)):\n",
    "                class_id = int(yolov9_preds[i, 0])\n",
    "                # Boost 'cast' and 'mycete' classes where YOLOv9 is significantly better\n",
    "                if class_id == 0:  # cast\n",
    "                    yolov9_preds[i, 5] *= 1.1\n",
    "                elif class_id == 6:  # mycete\n",
    "                    yolov9_preds[i, 5] *= 1.05\n",
    "            \n",
    "            # Boost certain classes for KD-ViT where it performs better\n",
    "            for i in range(len(kdvit_preds)):\n",
    "                class_id = int(kdvit_preds[i, 0])\n",
    "                # Boost 'leuko' class where KD-ViT is better\n",
    "                if class_id == 5:  # leuko\n",
    "                    kdvit_preds[i, 5] *= 1.1\n",
    "            \n",
    "            # Apply fusion method\n",
    "            if use_wbf:\n",
    "                ensemble_preds = weighted_box_fusion(\n",
    "                    [yolov9_preds, kdvit_preds],\n",
    "                    model_weights,\n",
    "                    iou_thresh,\n",
    "                    conf_thresh\n",
    "                )\n",
    "            else:\n",
    "                # Combine with confidence-weighted approach\n",
    "                if len(yolov9_preds) == 0 and len(kdvit_preds) == 0:\n",
    "                    ensemble_preds = np.empty((0, 6))\n",
    "                else:\n",
    "                    # Apply model-specific weights to confidence scores\n",
    "                    if len(yolov9_preds) > 0:\n",
    "                        yolov9_preds[:, 5] *= model_weights[0]\n",
    "                    if len(kdvit_preds) > 0:\n",
    "                        kdvit_preds[:, 5] *= model_weights[1]\n",
    "                    \n",
    "                    # Combine predictions\n",
    "                    combined_preds = np.vstack((yolov9_preds, kdvit_preds)) if len(yolov9_preds) > 0 and len(kdvit_preds) > 0 else (\n",
    "                        yolov9_preds if len(yolov9_preds) > 0 else kdvit_preds\n",
    "                    )\n",
    "                    \n",
    "                    # Apply Soft-NMS\n",
    "                    ensemble_preds = soft_weighted_nms(combined_preds, iou_thresh)\n",
    "            \n",
    "            # Save results\n",
    "            np.savetxt(save_path, ensemble_preds, fmt='%.6f')\n",
    "            #print(f\"✅ Advanced ensemble saved: {file} | Detections: {len(ensemble_preds)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {str(e)}\")\n",
    "\n",
    "# === Configuration ===\n",
    "# Folders\n",
    "model1_dir = 'C:/Mansura/UTI-Revision2/NMS/yolov10_corners'\n",
    "model2_dir = 'C:/Mansura/UTI-Revision2/NMS/kdvit_corners'\n",
    "ensemble_dir = 'C:/Mansura/UTI-Revision2/NMS/advanced_ensemble_output'\n",
    "\n",
    "# Model weights - give more weight to YOLOv9 as it performs better overall\n",
    "model_weights = [0.6, 0.4]  # [YOLOv9, KD-ViT]\n",
    "\n",
    "# === Run Advanced Ensemble ===\n",
    "advanced_ensemble(\n",
    "    model1_dir, \n",
    "    model2_dir, \n",
    "    ensemble_dir,\n",
    "    iou_thresh=0.5, \n",
    "    conf_thresh=0.1,\n",
    "    model_weights=model_weights,\n",
    "    use_wbf=True  # Set to True for Weighted Box Fusion, False for Soft-NMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae9617",
   "metadata": {},
   "source": [
    "nms and soft-nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20bcc01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting NMS vs Soft-NMS Comparison ===\n",
      "\n",
      "=== Comparison Complete ===\n",
      "Standard NMS found 20876 detections across 852 files\n",
      "Soft-NMS found 8983 detections across 852 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate IoU between two boxes [x1, y1, x2, y2]\"\"\"\n",
    "    # Calculate intersection\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    # Check if boxes overlap\n",
    "    if x2 < x1 or y2 < y1:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = (x2 - x1) * (y2 - y1)\n",
    "    \n",
    "    # Calculate union\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = area1 + area2 - intersection\n",
    "    \n",
    "    # Calculate IoU\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "def standard_nms(predictions, iou_thresh=0.5, score_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Apply standard NMS to predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predictions [class_id, x1, y1, x2, y2, conf]\n",
    "    - iou_thresh: IoU threshold for NMS\n",
    "    - score_threshold: Minimum score threshold to keep a box\n",
    "    \n",
    "    Returns:\n",
    "    - Filtered predictions\n",
    "    \"\"\"\n",
    "    if len(predictions) == 0:\n",
    "        return np.empty((0, 6))\n",
    "    \n",
    "    # Group by class\n",
    "    class_groups = defaultdict(list)\n",
    "    for pred in predictions:\n",
    "        class_groups[int(pred[0])].append(pred)\n",
    "    \n",
    "    final_predictions = []\n",
    "    \n",
    "    for class_id, preds in class_groups.items():\n",
    "        preds = np.array(preds)\n",
    "        if len(preds) == 1:\n",
    "            final_predictions.append(preds[0])\n",
    "            continue\n",
    "            \n",
    "        # Sort by confidence score\n",
    "        order = np.argsort(-preds[:, 5])\n",
    "        preds = preds[order]\n",
    "        \n",
    "        keep = []\n",
    "        while len(preds) > 0:\n",
    "            # Keep highest score box\n",
    "            keep.append(preds[0])\n",
    "            \n",
    "            # Exit if no more boxes\n",
    "            if len(preds) == 1:\n",
    "                break\n",
    "                \n",
    "            # Calculate IoU of first box with all others\n",
    "            ious = []\n",
    "            for i in range(1, len(preds)):\n",
    "                iou = calculate_iou(preds[0, 1:5], preds[i, 1:5])\n",
    "                ious.append(iou)\n",
    "            \n",
    "            # Keep only boxes with IoU less than threshold\n",
    "            mask = np.array(ious) < iou_thresh\n",
    "            preds = preds[1:][mask]\n",
    "    \n",
    "        final_predictions.extend(keep)\n",
    "    \n",
    "    if final_predictions:\n",
    "        return np.array(final_predictions)\n",
    "    else:\n",
    "        return np.empty((0, 6))\n",
    "\n",
    "def soft_nms(predictions, method='gaussian', iou_thresh=0.5, sigma=0.5, score_threshold=0.001):\n",
    "    \"\"\"\n",
    "    Apply Soft-NMS to predictions\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions: numpy array of predictions [class_id, x1, y1, x2, y2, conf]\n",
    "    - method: 'gaussian' or 'linear' penalty function\n",
    "    - iou_thresh: IoU threshold for NMS\n",
    "    - sigma: Parameter for Gaussian penalty function\n",
    "    - score_threshold: Minimum score threshold to keep a box\n",
    "    \n",
    "    Returns:\n",
    "    - Filtered predictions\n",
    "    \"\"\"\n",
    "    if len(predictions) == 0:\n",
    "        return np.empty((0, 6))\n",
    "    \n",
    "    # Group by class\n",
    "    class_groups = defaultdict(list)\n",
    "    for pred in predictions:\n",
    "        class_groups[int(pred[0])].append(pred)\n",
    "    \n",
    "    final_predictions = []\n",
    "    \n",
    "    for class_id, preds in class_groups.items():\n",
    "        preds = np.array(preds)\n",
    "        if len(preds) == 1:\n",
    "            final_predictions.append(preds[0])\n",
    "            continue\n",
    "            \n",
    "        # Sort by confidence score\n",
    "        order = np.argsort(-preds[:, 5])\n",
    "        preds = preds[order]\n",
    "        \n",
    "        boxes = preds[:, 1:5]\n",
    "        scores = preds[:, 5].copy()\n",
    "        \n",
    "        keep = []\n",
    "        \n",
    "        while len(scores) > 0:\n",
    "            # Save highest scoring box\n",
    "            if scores[0] >= score_threshold:\n",
    "                # Save the original prediction with updated score\n",
    "                new_pred = preds[0].copy()\n",
    "                new_pred[5] = scores[0]\n",
    "                keep.append(new_pred)\n",
    "                \n",
    "            # Break if only one box remains\n",
    "            if len(scores) == 1:\n",
    "                break\n",
    "                \n",
    "            # Get first box\n",
    "            first_box = boxes[0]\n",
    "            \n",
    "            # Calculate IoU with all remaining boxes\n",
    "            ious = []\n",
    "            for i in range(1, len(boxes)):\n",
    "                iou = calculate_iou(first_box, boxes[i])\n",
    "                ious.append(iou)\n",
    "            \n",
    "            ious = np.array(ious)\n",
    "            \n",
    "            # Apply penalty to scores based on IoU\n",
    "            for i in range(1, len(scores)):\n",
    "                if ious[i-1] > iou_thresh:\n",
    "                    if method == 'gaussian':\n",
    "                        # Gaussian penalty\n",
    "                        scores[i] *= np.exp(-(ious[i-1] * ious[i-1]) / sigma)\n",
    "                    elif method == 'linear':\n",
    "                        # Linear penalty\n",
    "                        scores[i] *= (1 - ious[i-1])\n",
    "            \n",
    "            # Remove first box\n",
    "            boxes = boxes[1:]\n",
    "            scores = scores[1:]\n",
    "            preds = preds[1:]\n",
    "            \n",
    "            # Re-sort by updated scores\n",
    "            order = np.argsort(-scores)\n",
    "            boxes = boxes[order]\n",
    "            scores = scores[order]\n",
    "            preds = preds[order]\n",
    "    \n",
    "        final_predictions.extend(keep)\n",
    "    \n",
    "    if final_predictions:\n",
    "        return np.array(final_predictions)\n",
    "    else:\n",
    "        return np.empty((0, 6))\n",
    "\n",
    "def combine_predictions(yolo_preds, kdvit_preds, model_weights=[0.6, 0.4]):\n",
    "    \"\"\"Combine predictions from two models with class-specific weighting\"\"\"\n",
    "    if len(yolo_preds) > 0:\n",
    "        # Class-specific weighting for YOLOv9\n",
    "        for i in range(len(yolo_preds)):\n",
    "            class_id = int(yolo_preds[i, 0])\n",
    "            # Boost 'cast' and 'mycete' classes where YOLOv9 is significantly better\n",
    "            if class_id == 0:  # cast\n",
    "                yolo_preds[i, 5] *= 1.1\n",
    "            elif class_id == 6:  # mycete\n",
    "                yolo_preds[i, 5] *= 1.05\n",
    "        \n",
    "        # Apply model weight\n",
    "        yolo_preds[:, 5] *= model_weights[0]\n",
    "    \n",
    "    if len(kdvit_preds) > 0:\n",
    "        # Boost certain classes for KD-ViT where it performs better\n",
    "        for i in range(len(kdvit_preds)):\n",
    "            class_id = int(kdvit_preds[i, 0])\n",
    "            # Boost 'leuko' class where KD-ViT is better\n",
    "            if class_id == 5:  # leuko\n",
    "                kdvit_preds[i, 5] *= 1.1\n",
    "        \n",
    "        # Apply model weight\n",
    "        kdvit_preds[:, 5] *= model_weights[1]\n",
    "    \n",
    "    # Combine predictions\n",
    "    if len(yolo_preds) == 0 and len(kdvit_preds) == 0:\n",
    "        return np.empty((0, 6))\n",
    "    elif len(yolo_preds) == 0:\n",
    "        return kdvit_preds\n",
    "    elif len(kdvit_preds) == 0:\n",
    "        return yolo_preds\n",
    "    else:\n",
    "        return np.vstack((yolo_preds, kdvit_preds))\n",
    "\n",
    "def process_files(yolo_folder, kdvit_folder, nms_output, soft_nms_output, \n",
    "                 iou_thresh=0.5, sigma=0.5, conf_thresh=0.1):\n",
    "    \"\"\"\n",
    "    Process all files applying both NMS and Soft-NMS for comparison\n",
    "    \n",
    "    Parameters:\n",
    "    - yolo_folder: Directory containing YOLOv9 predictions\n",
    "    - kdvit_folder: Directory containing KD-YOLOX-ViT predictions\n",
    "    - nms_output: Directory to save standard NMS results\n",
    "    - soft_nms_output: Directory to save Soft-NMS results\n",
    "    - iou_thresh: IoU threshold for NMS\n",
    "    - sigma: Parameter for Gaussian penalty function in Soft-NMS\n",
    "    - conf_thresh: Confidence threshold for filtering weak predictions\n",
    "    \"\"\"\n",
    "    os.makedirs(nms_output, exist_ok=True)\n",
    "    os.makedirs(soft_nms_output, exist_ok=True)\n",
    "    \n",
    "    # Get files from yolo folder\n",
    "    files = [f for f in os.listdir(yolo_folder) if f.endswith('.txt')]\n",
    "    \n",
    "    nms_stats = {'files': 0, 'total_detections': 0, 'time': 0}\n",
    "    soft_nms_stats = {'files': 0, 'total_detections': 0, 'time': 0}\n",
    "    \n",
    "    for file in files:\n",
    "        yolov9_path = os.path.join(yolo_folder, file)\n",
    "        kdvit_path = os.path.join(kdvit_folder, file)\n",
    "        nms_save_path = os.path.join(nms_output, file)\n",
    "        soft_nms_save_path = os.path.join(soft_nms_output, file)\n",
    "        \n",
    "        # Skip if kdvit prediction doesn't exist\n",
    "        if not os.path.exists(kdvit_path):\n",
    "            print(f\"⚠️ Missing KD-ViT prediction for {file}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Load predictions\n",
    "            yolov9_preds = np.loadtxt(yolov9_path).reshape(-1, 6) if os.path.getsize(yolov9_path) > 0 else np.empty((0, 6))\n",
    "            kdvit_preds = np.loadtxt(kdvit_path).reshape(-1, 6) if os.path.getsize(kdvit_path) > 0 else np.empty((0, 6))\n",
    "            \n",
    "            # Handle empty files or single detection\n",
    "            if yolov9_preds.size > 0 and yolov9_preds.ndim == 1:\n",
    "                yolov9_preds = yolov9_preds.reshape(1, -1)\n",
    "            if kdvit_preds.size > 0 and kdvit_preds.ndim == 1:\n",
    "                kdvit_preds = kdvit_preds.reshape(1, -1)\n",
    "            \n",
    "            # Combine predictions from both models\n",
    "            combined_preds = combine_predictions(\n",
    "                yolov9_preds.copy() if len(yolov9_preds) > 0 else np.empty((0, 6)), \n",
    "                kdvit_preds.copy() if len(kdvit_preds) > 0 else np.empty((0, 6))\n",
    "            )\n",
    "            \n",
    "            # Apply standard NMS\n",
    "            start_time = time.time()\n",
    "            nms_preds = standard_nms(\n",
    "                combined_preds,\n",
    "                iou_thresh=iou_thresh,\n",
    "                score_threshold=conf_thresh\n",
    "            )\n",
    "            nms_time = time.time() - start_time\n",
    "            \n",
    "            # Apply Soft-NMS\n",
    "            start_time = time.time()\n",
    "            soft_nms_preds = soft_nms(\n",
    "                combined_preds,\n",
    "                method='gaussian',\n",
    "                iou_thresh=iou_thresh,\n",
    "                sigma=sigma,\n",
    "                score_threshold=conf_thresh\n",
    "            )\n",
    "            soft_nms_time = time.time() - start_time\n",
    "            \n",
    "            # Save results\n",
    "            np.savetxt(nms_save_path, nms_preds, fmt='%.6f')\n",
    "            np.savetxt(soft_nms_save_path, soft_nms_preds, fmt='%.6f')\n",
    "            \n",
    "            # Update stats\n",
    "            nms_stats['files'] += 1\n",
    "            nms_stats['total_detections'] += len(nms_preds)\n",
    "            nms_stats['time'] += nms_time\n",
    "            \n",
    "            soft_nms_stats['files'] += 1\n",
    "            soft_nms_stats['total_detections'] += len(soft_nms_preds)\n",
    "            soft_nms_stats['time'] += soft_nms_time\n",
    "            \n",
    "            #print(f\"✅ Processed {file} | NMS: {len(nms_preds)} detections | Soft-NMS: {len(soft_nms_preds)} detections\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {str(e)}\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    if nms_stats['files'] > 0:\n",
    "        nms_stats['avg_detections'] = nms_stats['total_detections'] / nms_stats['files']\n",
    "        nms_stats['avg_time'] = nms_stats['time'] / nms_stats['files']\n",
    "        \n",
    "    if soft_nms_stats['files'] > 0:\n",
    "        soft_nms_stats['avg_detections'] = soft_nms_stats['total_detections'] / soft_nms_stats['files']\n",
    "        soft_nms_stats['avg_time'] = soft_nms_stats['time'] / soft_nms_stats['files']\n",
    "    \n",
    "    return nms_stats, soft_nms_stats\n",
    "\n",
    "def generate_comparison_report(nms_stats, soft_nms_stats, output_file):\n",
    "    \"\"\"Generate a comparison report between NMS and Soft-NMS\"\"\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"# NMS vs Soft-NMS Comparison Report\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Performance Statistics\\n\\n\")\n",
    "        f.write(\"| Metric | Standard NMS | Soft-NMS |\\n\")\n",
    "        f.write(\"|--------|-------------|----------|\\n\")\n",
    "        f.write(f\"| Files Processed | {nms_stats['files']} | {soft_nms_stats['files']} |\\n\")\n",
    "        f.write(f\"| Total Detections | {nms_stats['total_detections']} | {soft_nms_stats['total_detections']} |\\n\")\n",
    "        f.write(f\"| Average Detections per File | {nms_stats['avg_detections']:.2f} | {soft_nms_stats['avg_detections']:.2f} |\\n\")\n",
    "        f.write(f\"| Total Processing Time (s) | {nms_stats['time']:.4f} | {soft_nms_stats['time']:.4f} |\\n\")\n",
    "        f.write(f\"| Average Processing Time per File (s) | {nms_stats['avg_time']:.6f} | {soft_nms_stats['avg_time']:.6f} |\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Analysis\\n\\n\")\n",
    "        \n",
    "        # Detection difference percentage\n",
    "        detection_diff_pct = ((soft_nms_stats['total_detections'] - nms_stats['total_detections']) / \n",
    "                             nms_stats['total_detections'] * 100) if nms_stats['total_detections'] > 0 else 0\n",
    "        \n",
    "        f.write(f\"### Detection Count Analysis\\n\\n\")\n",
    "        f.write(f\"Soft-NMS detected {detection_diff_pct:.2f}% \")\n",
    "        if detection_diff_pct > 0:\n",
    "            f.write(\"more objects than standard NMS.\\n\\n\")\n",
    "        elif detection_diff_pct < 0:\n",
    "            f.write(\"fewer objects than standard NMS.\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"the same number of objects as standard NMS.\\n\\n\")\n",
    "        \n",
    "        # Time comparison\n",
    "        time_diff_pct = ((soft_nms_stats['time'] - nms_stats['time']) / \n",
    "                        nms_stats['time'] * 100) if nms_stats['time'] > 0 else 0\n",
    "        \n",
    "        f.write(f\"### Processing Time Analysis\\n\\n\")\n",
    "        f.write(f\"Soft-NMS was {abs(time_diff_pct):.2f}% \")\n",
    "        if time_diff_pct > 0:\n",
    "            f.write(\"slower than standard NMS.\\n\\n\")\n",
    "        elif time_diff_pct < 0:\n",
    "            f.write(\"faster than standard NMS.\\n\\n\")\n",
    "        else:\n",
    "            f.write(\"the same speed as standard NMS.\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Key Differences\\n\\n\")\n",
    "        f.write(\"- **Standard NMS** completely removes overlapping boxes, which can be a problem for objects that are close together.\\n\")\n",
    "        f.write(\"- **Soft-NMS** reduces the confidence of overlapping boxes instead of removing them completely.\\n\")\n",
    "        f.write(\"- This usually results in **better recall** for Soft-NMS, especially in crowded scenes.\\n\")\n",
    "        f.write(\"- Soft-NMS typically preserves more detections in areas where objects overlap.\\n\\n\")\n",
    "        \n",
    "        f.write(\"### Recommendations\\n\\n\")\n",
    "        f.write(\"- Use **Standard NMS** when:\\n\")\n",
    "        f.write(\"  - Processing speed is critical\\n\")\n",
    "        f.write(\"  - Objects are well-separated\\n\")\n",
    "        f.write(\"  - False positives are a concern\\n\\n\")\n",
    "        f.write(\"- Use **Soft-NMS** when:\\n\")\n",
    "        f.write(\"  - Objects frequently overlap\\n\")\n",
    "        f.write(\"  - Recall is more important than precision\\n\")\n",
    "        f.write(\"  - You're working with crowded scenes\\n\")\n",
    "        f.write(\"  - Missing detections is more problematic than having duplicate detections\\n\")\n",
    "\n",
    "def plot_comparison(nms_stats, soft_nms_stats, output_image):\n",
    "    \"\"\"Create comparison plots between NMS and Soft-NMS\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Average Detections\n",
    "    methods = ['Standard NMS', 'Soft-NMS']\n",
    "    avg_detections = [nms_stats['avg_detections'], soft_nms_stats['avg_detections']]\n",
    "    \n",
    "    ax1.bar(methods, avg_detections, color=['blue', 'green'])\n",
    "    ax1.set_title('Average Detections per File')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for i, v in enumerate(avg_detections):\n",
    "        ax1.text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "    \n",
    "    # Plot 2: Average Processing Time\n",
    "    avg_times = [nms_stats['avg_time'], soft_nms_stats['avg_time']]\n",
    "    \n",
    "    ax2.bar(methods, avg_times, color=['blue', 'green'])\n",
    "    ax2.set_title('Average Processing Time per File')\n",
    "    ax2.set_ylabel('Time (seconds)')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for i, v in enumerate(avg_times):\n",
    "        ax2.text(i, v + 0.0001, f\"{v:.6f}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_image)\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === Configuration ===\n",
    "    # Folders\n",
    "    model1_dir = 'C:/Mansura/UTI-Revision2/NMS/yolov10_corners'\n",
    "    model2_dir = 'C:/Mansura/UTI-Revision2/NMS/kdvit_corners'\n",
    "    standard_nms_dir = 'C:/Mansura/UTI-Revision2/NMS/standard_nms_output'\n",
    "    soft_nms_dir = 'C:/Mansura/UTI-Revision2/NMS/soft_nms_output'\n",
    "    \n",
    "    # Report outputs\n",
    "    #report_file = 'C:/Mansura/UTI-Revision2/NMS/nms_comparison_report.md'\n",
    "    #comparison_plot = 'C:/Mansura/UTI-Revision2/NMS/nms_comparison_plot.png'\n",
    "    \n",
    "    print(\"\\n=== Starting NMS vs Soft-NMS Comparison ===\")\n",
    "    \n",
    "    # Process files with both methods\n",
    "    nms_stats, soft_nms_stats = process_files(\n",
    "        model1_dir,\n",
    "        model2_dir,\n",
    "        standard_nms_dir,\n",
    "        soft_nms_dir,\n",
    "        iou_thresh=0.5,\n",
    "        sigma=0.5,\n",
    "        conf_thresh=0.1\n",
    "    )\n",
    "    \n",
    "    # Generate comparison report\n",
    "    #generate_comparison_report(nms_stats, soft_nms_stats, report_file)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    #plot_comparison(nms_stats, soft_nms_stats, comparison_plot)\n",
    "    \n",
    "    print(f\"\\n=== Comparison Complete ===\")\n",
    "    print(f\"Standard NMS found {nms_stats['total_detections']} detections across {nms_stats['files']} files\")\n",
    "    print(f\"Soft-NMS found {soft_nms_stats['total_detections']} detections across {soft_nms_stats['files']} files\")\n",
    "    #print(f\"Detailed report saved to: {report_file}\")\n",
    "    #print(f\"Comparison plot saved to: {comparison_plot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681f1df",
   "metadata": {},
   "source": [
    "mAP calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b39b4c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class-wise Average Precision:\n",
      "--------------------------------------------------\n",
      "cast       - AP: 0.7898, GT count: 545\n",
      "cryst      - AP: 0.8772, GT count: 317\n",
      "epith      - AP: 0.8882, GT count: 972\n",
      "epithn     - AP: 0.9263, GT count: 77\n",
      "eryth      - AP: 0.9440, GT count: 3008\n",
      "leuko      - AP: 0.9295, GT count: 796\n",
      "mycete     - AP: 0.8973, GT count: 233\n",
      "--------------------------------------------------\n",
      "Final mAP@50: 0.8932\n",
      "Precision-recall curves saved to pr_curves.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between two bounding boxes\n",
    "    \n",
    "    Args:\n",
    "        box1: [x1, y1, x2, y2]\n",
    "        box2: [x1, y1, x2, y2]\n",
    "    \n",
    "    Returns:\n",
    "        iou: intersection over union\n",
    "    \"\"\"\n",
    "    # Get coordinates of intersection\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    \n",
    "    # Calculate area of intersection\n",
    "    width_inter = max(0, x2_inter - x1_inter)\n",
    "    height_inter = max(0, y2_inter - y1_inter)\n",
    "    area_inter = width_inter * height_inter\n",
    "    \n",
    "    # Calculate area of both boxes\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # Calculate area of union\n",
    "    area_union = area_box1 + area_box2 - area_inter\n",
    "    \n",
    "    # Return IoU\n",
    "    if area_union > 0:\n",
    "        return area_inter / area_union\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_map50_coco(pred_folder, gt_folder, iou_thresh=0.5, conf_thresh=0.001):\n",
    "    \"\"\"\n",
    "    Calculate mAP@50 for predictions against ground truth using COCO-style AP\n",
    "    \n",
    "    Args:\n",
    "        pred_folder: folder containing prediction files in format [class_id, x1, y1, x2, y2, confidence]\n",
    "        gt_folder: folder containing ground truth files in format [class_id, x1, y1, x2, y2]\n",
    "        iou_thresh: IoU threshold for considering a prediction as correct\n",
    "        conf_thresh: Confidence threshold for filtering predictions\n",
    "    \n",
    "    Returns:\n",
    "        mAP@50: mean Average Precision at IoU threshold of 0.5\n",
    "    \"\"\"\n",
    "    # Dictionary to store all predictions for each class\n",
    "    all_predictions = defaultdict(list)\n",
    "    # Dictionary to store ground truth count for each class\n",
    "    gt_counter_per_class = defaultdict(int)\n",
    "    \n",
    "    # Map class_ids to class names if available\n",
    "    class_names = {\n",
    "        0: \"cast\",\n",
    "        1: \"cryst\",\n",
    "        2: \"epith\",\n",
    "        3: \"epithn\",\n",
    "        4: \"eryth\",\n",
    "        5: \"leuko\",\n",
    "        6: \"mycete\"\n",
    "    }\n",
    "    \n",
    "    files = [f for f in os.listdir(pred_folder) if f.endswith('.txt') and os.path.exists(os.path.join(gt_folder, f))]\n",
    "    \n",
    "    # Process each file\n",
    "    for file in files:\n",
    "        # Load ground truth\n",
    "        gt_path = os.path.join(gt_folder, file)\n",
    "        if os.path.getsize(gt_path) > 0:\n",
    "            gt_data = np.loadtxt(gt_path, ndmin=2)  # ndmin=2 ensures it's 2D even with single detection\n",
    "        else:\n",
    "            gt_data = np.empty((0, 5))\n",
    "            \n",
    "        # Load predictions\n",
    "        pred_path = os.path.join(pred_folder, file)\n",
    "        if os.path.getsize(pred_path) > 0:\n",
    "            pred_data = np.loadtxt(pred_path, ndmin=2)  # ndmin=2 ensures it's 2D even with single detection\n",
    "        else:\n",
    "            pred_data = np.empty((0, 6))\n",
    "        \n",
    "        # Process ground truth for this image\n",
    "        gt_this_image = {}\n",
    "        for gt_box in gt_data:\n",
    "            class_id = int(gt_box[0])\n",
    "            # Increment ground truth count for this class\n",
    "            gt_counter_per_class[class_id] += 1\n",
    "            \n",
    "            # Add ground truth box to dictionary, format: [used_flag, x1, y1, x2, y2]\n",
    "            # used_flag: whether this gt box has been matched with a prediction\n",
    "            if class_id not in gt_this_image:\n",
    "                gt_this_image[class_id] = []\n",
    "                \n",
    "            gt_this_image[class_id].append([False, gt_box[1], gt_box[2], gt_box[3], gt_box[4]])\n",
    "        \n",
    "        # Process predictions for this image\n",
    "        for pred_box in pred_data:\n",
    "            class_id = int(pred_box[0])\n",
    "            confidence = float(pred_box[5])\n",
    "            \n",
    "            # Skip predictions below confidence threshold\n",
    "            if confidence < conf_thresh:\n",
    "                continue\n",
    "                \n",
    "            pred_bbox = [float(x) for x in pred_box[1:5]]  # x1, y1, x2, y2\n",
    "            \n",
    "            # Add prediction to all_predictions list\n",
    "            # Format: [file_name, confidence, x1, y1, x2, y2, tp/fp]\n",
    "            # tp/fp: whether this prediction is true positive or false positive, initially set to False (fp)\n",
    "            all_predictions[class_id].append([file, confidence] + pred_bbox + [False])\n",
    "            \n",
    "            # If there are ground truths for this class in this image\n",
    "            if class_id in gt_this_image and len(gt_this_image[class_id]) > 0:\n",
    "                # Find the ground truth box with highest IoU\n",
    "                max_iou = -1\n",
    "                max_idx = -1\n",
    "                for idx, gt_box in enumerate(gt_this_image[class_id]):\n",
    "                    # Skip if this ground truth box has already been matched\n",
    "                    if gt_box[0]:\n",
    "                        continue\n",
    "                        \n",
    "                    # Calculate IoU\n",
    "                    iou = calculate_iou(pred_bbox, gt_box[1:])\n",
    "                    \n",
    "                    # Update if this IoU is higher\n",
    "                    if iou > max_iou:\n",
    "                        max_iou = iou\n",
    "                        max_idx = idx\n",
    "                \n",
    "                # If we found a match with IoU > threshold\n",
    "                if max_iou >= iou_thresh and max_idx >= 0:\n",
    "                    # Mark this ground truth box as used\n",
    "                    gt_this_image[class_id][max_idx][0] = True\n",
    "                    # Mark this prediction as true positive\n",
    "                    all_predictions[class_id][-1][-1] = True\n",
    "    \n",
    "    # Calculate AP for each class using COCO-style AP\n",
    "    sum_ap = 0\n",
    "    ap_dictionary = {}\n",
    "    valid_classes = 0\n",
    "    \n",
    "    print(\"\\nClass-wise Average Precision:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Process each class\n",
    "    for class_id in sorted(gt_counter_per_class.keys()):\n",
    "        # If no ground truth exists for this class\n",
    "        if gt_counter_per_class[class_id] == 0:\n",
    "            continue\n",
    "            \n",
    "        # If no predictions for this class\n",
    "        if class_id not in all_predictions:\n",
    "            ap_dictionary[class_id] = 0.0\n",
    "            sum_ap += 0.0\n",
    "            valid_classes += 1\n",
    "            class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
    "            print(f\"{class_name:<10} - AP: 0.0000, GT count: {gt_counter_per_class[class_id]}\")\n",
    "            continue\n",
    "            \n",
    "        # Sort predictions by confidence\n",
    "        predictions = all_predictions[class_id]\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Initialize true positives and false positives array\n",
    "        tp = np.array([pred[-1] for pred in predictions], dtype=np.float64)\n",
    "        fp = np.logical_not(tp).astype(np.float64)\n",
    "        \n",
    "        # Calculate cumulative false positives and true positives\n",
    "        cumsum_fp = np.cumsum(fp)\n",
    "        cumsum_tp = np.cumsum(tp)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision = cumsum_tp / (cumsum_fp + cumsum_tp + 1e-10)\n",
    "        recall = cumsum_tp / gt_counter_per_class[class_id]\n",
    "        \n",
    "        # Ensure precision is monotonically decreasing (COCO method)\n",
    "        for i in range(len(precision) - 2, -1, -1):\n",
    "            precision[i] = max(precision[i], precision[i + 1])\n",
    "            \n",
    "        # Find all unique recall points\n",
    "        recall_points = np.concatenate(([0], recall, [1]))\n",
    "        recall_points = np.unique(recall_points)\n",
    "        \n",
    "        # Interpolate precision at each recall point\n",
    "        interpolated_precision = np.zeros_like(recall_points)\n",
    "        \n",
    "        for i, r in enumerate(recall_points):\n",
    "            # Precision at recall >= r\n",
    "            precisions_at_recall = precision[recall >= r]\n",
    "            if len(precisions_at_recall) > 0:\n",
    "                interpolated_precision[i] = np.max(precisions_at_recall)\n",
    "        \n",
    "        # Calculate AP as area under precision-recall curve\n",
    "        ap = np.sum((recall_points[1:] - recall_points[:-1]) * interpolated_precision[:-1])\n",
    "            \n",
    "        # Store AP for this class\n",
    "        ap_dictionary[class_id] = ap\n",
    "        sum_ap += ap\n",
    "        valid_classes += 1\n",
    "        \n",
    "        class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
    "        print(f\"{class_name:<10} - AP: {ap:.4f}, GT count: {gt_counter_per_class[class_id]}\")\n",
    "        \n",
    "    # Calculate mAP\n",
    "    if valid_classes > 0:\n",
    "        mAP = sum_ap / valid_classes\n",
    "    else:\n",
    "        mAP = 0.0\n",
    "        \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Final mAP@50: {mAP:.4f}\")\n",
    "        \n",
    "    return mAP\n",
    "\n",
    "def plot_precision_recall_curves(pred_folder, gt_folder, iou_thresh=0.5, conf_thresh=0.001, output_file='pr_curves.png'):\n",
    "    \"\"\"\n",
    "    Plot precision-recall curves for each class using COCO-style AP calculation\n",
    "    \"\"\"\n",
    "    # Dictionary to store all predictions for each class\n",
    "    all_predictions = defaultdict(list)\n",
    "    # Dictionary to store ground truth count for each class\n",
    "    gt_counter_per_class = defaultdict(int)\n",
    "    \n",
    "    # Map class_ids to class names if available\n",
    "    class_names = {\n",
    "        0: \"cast\",\n",
    "        1: \"cryst\",\n",
    "        2: \"epith\",\n",
    "        3: \"epithn\",\n",
    "        4: \"eryth\",\n",
    "        5: \"leuko\",\n",
    "        6: \"mycete\"\n",
    "    }\n",
    "    \n",
    "    files = [f for f in os.listdir(pred_folder) if f.endswith('.txt') and os.path.exists(os.path.join(gt_folder, f))]\n",
    "    \n",
    "    # Process each file (same as in evaluate_map50_coco)\n",
    "    # ... (same processing code as in evaluate_map50_coco)\n",
    "    \n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Process each class and plot its precision-recall curve\n",
    "    for class_id in all_predictions:\n",
    "        if gt_counter_per_class[class_id] == 0:\n",
    "            continue\n",
    "            \n",
    "        # Sort predictions by confidence\n",
    "        predictions = all_predictions[class_id]\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Get tp/fp arrays\n",
    "        tp = np.array([pred[-1] for pred in predictions], dtype=np.float64)\n",
    "        fp = np.logical_not(tp).astype(np.float64)\n",
    "        \n",
    "        # Calculate cumulative values\n",
    "        cumsum_fp = np.cumsum(fp)\n",
    "        cumsum_tp = np.cumsum(tp)\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision = cumsum_tp / (cumsum_fp + cumsum_tp + 1e-10)\n",
    "        recall = cumsum_tp / gt_counter_per_class[class_id]\n",
    "        \n",
    "        # Ensure precision is monotonically decreasing (COCO method)\n",
    "        for i in range(len(precision) - 2, -1, -1):\n",
    "            precision[i] = max(precision[i], precision[i + 1])\n",
    "        \n",
    "        # Calculate AP\n",
    "        # Find all unique recall points\n",
    "        recall_points = np.concatenate(([0], recall, [1]))\n",
    "        recall_points = np.unique(recall_points)\n",
    "        \n",
    "        # Interpolate precision at each recall point\n",
    "        interpolated_precision = np.zeros_like(recall_points)\n",
    "        \n",
    "        for i, r in enumerate(recall_points):\n",
    "            # Precision at recall >= r\n",
    "            precisions_at_recall = precision[recall >= r]\n",
    "            if len(precisions_at_recall) > 0:\n",
    "                interpolated_precision[i] = np.max(precisions_at_recall)\n",
    "        \n",
    "        # Calculate AP as area under precision-recall curve\n",
    "        ap = np.sum((recall_points[1:] - recall_points[:-1]) * interpolated_precision[:-1])\n",
    "        \n",
    "        # Plot precision-recall curve\n",
    "        class_name = class_names.get(class_id, f\"Class {class_id}\")\n",
    "        plt.plot(recall, precision, '-', label=f'{class_name} (AP: {ap:.4f})')\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curves (IoU={iou_thresh}, Conf={conf_thresh})')\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.ylim(0, 1.01)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower left')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(output_file)\n",
    "    print(f\"Precision-recall curves saved to {output_file}\")\n",
    "    plt.close()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    gt_folder = 'C:/Mansura/UTI-Revision2/NMS/gt_corners'\n",
    "    pred_folder = 'C:/Mansura/UTI-Revision2/NMS/soft_nms_output'\n",
    "    \n",
    "    # Calculate mAP@50 using COCO method with same confidence threshold as YOLO (0.001)\n",
    "    map50 = evaluate_map50_coco(pred_folder, gt_folder, iou_thresh=0.5, conf_thresh=0.001)\n",
    "    \n",
    "    # Optionally plot precision-recall curves\n",
    "    plot_precision_recall_curves(pred_folder, gt_folder, iou_thresh=0.5, conf_thresh=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
